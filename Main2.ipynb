{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>Music recommendation using graphs</h1>\n",
    "<h2>MLNS PROJECT</h2>\n",
    "<h3>Coded by Chloé Daems, Amir Mahmoudi and Anne-Claire Laisney</h3>\n",
    "</center>\n",
    "\n",
    "This is the main notebook to create a benchmark of graph based music recommendation systems inspired by the *Katarya, R., Verma, O.P. Efficient music recommender system using context graph and particle swarm. Multimed Tools Appl 77, 2673–2687 (2018).* [paper](URL 'https://link.springer.com/article/10.1007/s11042-017-4447-x'), using data from the user.getRecentTracks of the [Last.fm](URL 'https://www.last.fm/api/show/user.getRecentTracks') API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the libraries\n",
    "from os.path import exists\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import datetime\n",
    "from scipy.sparse import *\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
    "\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/numpy/lib/arraysetops.py:580: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    }
   ],
   "source": [
    "user_id_profile = pd.read_csv('lastfm-dataset-1K/userid-profile.tsv', sep = '\\t')\n",
    "if not exists('lastfm-dataset-1K/user_id_logs_v2.tsv'):\n",
    "    logs_columns = ['userid', 'timestamp', 'artist-id', 'artist-name', 'track-id', 'track-name']\n",
    "    user_id_logs = pd.read_csv('lastfm-dataset-1K/userid-logs.tsv', sep = '\\t', header = None, names =  logs_columns )\n",
    "    user_id_logs = user_id_logs.dropna(subset=['track-name','artist-name', 'artist-id'])\n",
    "else : \n",
    "    user_id_logs = pd.read_csv('lastfm-dataset-1K/user_id_logs_v2.tsv',index_col=0)\n",
    "    user_id_logs = user_id_logs.dropna(subset=['track-name','artist-name', 'artist-id'])\n",
    "    \n",
    "#user_id_logs['timestamp'] = pd.to_datetime(user_id_logs['timestamp'], format='%Y-%m-%dT%H:%M:%SZ')\n",
    "user_id_logs['timestamp'] = pd.to_datetime(user_id_logs['timestamp'], format='%Y-%m-%d')# %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#id</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>country</th>\n",
       "      <th>registered</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>user_000001</td>\n",
       "      <td>m</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Japan</td>\n",
       "      <td>Aug 13, 2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>user_000002</td>\n",
       "      <td>f</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Peru</td>\n",
       "      <td>Feb 24, 2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>user_000003</td>\n",
       "      <td>m</td>\n",
       "      <td>22.0</td>\n",
       "      <td>United States</td>\n",
       "      <td>Oct 30, 2005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>user_000004</td>\n",
       "      <td>f</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apr 26, 2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>user_000005</td>\n",
       "      <td>m</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Bulgaria</td>\n",
       "      <td>Jun 29, 2006</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           #id gender   age        country    registered\n",
       "0  user_000001      m   NaN          Japan  Aug 13, 2006\n",
       "1  user_000002      f   NaN           Peru  Feb 24, 2006\n",
       "2  user_000003      m  22.0  United States  Oct 30, 2005\n",
       "3  user_000004      f   NaN            NaN  Apr 26, 2006\n",
       "4  user_000005      m   NaN       Bulgaria  Jun 29, 2006"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_id_profile.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userid</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>artist-id</th>\n",
       "      <th>artist-name</th>\n",
       "      <th>track-id</th>\n",
       "      <th>track-name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>user_000001</td>\n",
       "      <td>2009-05-04 23:08:57</td>\n",
       "      <td>f1b1cf71-bd35-4e99-8624-24a6e15f133a</td>\n",
       "      <td>Deep Dish</td>\n",
       "      <td>7369ec4f-b377-5683-86bd-f02897317103</td>\n",
       "      <td>Fuck Me Im Famous (Pacha Ibiza)-09-28-2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>user_000001</td>\n",
       "      <td>2009-05-04 13:54:10</td>\n",
       "      <td>a7f7df4a-77d8-4f12-8acd-5c60c93f4de8</td>\n",
       "      <td>坂本龍一</td>\n",
       "      <td>8a0799b1-2f64-5e7b-9436-2228c9d65637</td>\n",
       "      <td>Composition 0919 (Live_2009_4_15)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>user_000001</td>\n",
       "      <td>2009-05-04 13:52:04</td>\n",
       "      <td>a7f7df4a-77d8-4f12-8acd-5c60c93f4de8</td>\n",
       "      <td>坂本龍一</td>\n",
       "      <td>44da66dc-6a34-54de-a4d9-686bc38ede0f</td>\n",
       "      <td>Mc2 (Live_2009_4_15)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>user_000001</td>\n",
       "      <td>2009-05-04 13:42:52</td>\n",
       "      <td>a7f7df4a-77d8-4f12-8acd-5c60c93f4de8</td>\n",
       "      <td>坂本龍一</td>\n",
       "      <td>e625acbe-1360-528d-8afe-4ad88424e0c0</td>\n",
       "      <td>Hibari (Live_2009_4_15)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>user_000001</td>\n",
       "      <td>2009-05-04 13:42:11</td>\n",
       "      <td>a7f7df4a-77d8-4f12-8acd-5c60c93f4de8</td>\n",
       "      <td>坂本龍一</td>\n",
       "      <td>fa332ed7-b701-5669-9e8e-0961658cdb43</td>\n",
       "      <td>Mc1 (Live_2009_4_15)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        userid           timestamp                             artist-id  \\\n",
       "0  user_000001 2009-05-04 23:08:57  f1b1cf71-bd35-4e99-8624-24a6e15f133a   \n",
       "1  user_000001 2009-05-04 13:54:10  a7f7df4a-77d8-4f12-8acd-5c60c93f4de8   \n",
       "2  user_000001 2009-05-04 13:52:04  a7f7df4a-77d8-4f12-8acd-5c60c93f4de8   \n",
       "3  user_000001 2009-05-04 13:42:52  a7f7df4a-77d8-4f12-8acd-5c60c93f4de8   \n",
       "4  user_000001 2009-05-04 13:42:11  a7f7df4a-77d8-4f12-8acd-5c60c93f4de8   \n",
       "\n",
       "  artist-name                              track-id  \\\n",
       "0   Deep Dish  7369ec4f-b377-5683-86bd-f02897317103   \n",
       "1        坂本龍一  8a0799b1-2f64-5e7b-9436-2228c9d65637   \n",
       "2        坂本龍一  44da66dc-6a34-54de-a4d9-686bc38ede0f   \n",
       "3        坂本龍一  e625acbe-1360-528d-8afe-4ad88424e0c0   \n",
       "4        坂本龍一  fa332ed7-b701-5669-9e8e-0961658cdb43   \n",
       "\n",
       "                                   track-name  \n",
       "0  Fuck Me Im Famous (Pacha Ibiza)-09-28-2007  \n",
       "1           Composition 0919 (Live_2009_4_15)  \n",
       "2                        Mc2 (Live_2009_4_15)  \n",
       "3                     Hibari (Live_2009_4_15)  \n",
       "4                        Mc1 (Live_2009_4_15)  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_id_logs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**There are too many track-ids missing, we are going to recreate them using the uuid library**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Really long : 40 min\n",
    "import tqdm\n",
    "import uuid\n",
    "if not exists('lastfm-dataset-1K/user_id_logs_v2.tsv'):\n",
    "    for idx, row in tqdm.tqdm(user_id_logs.iterrows()):\n",
    "        row['track-id'] = uuid.uuid5(uuid.NAMESPACE_DNS, row['artist-name'] + \",\" + row['track-name'])\n",
    "        row['artist-id'] = uuid.uuid5(uuid.NAMESPACE_DNS, str(row['artist-name']))\n",
    "    #We save the file\n",
    "    user_id_logs.to_csv('lastfm-dataset-1K/user_id_logs_v2.tsv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We create a train and test set**\n",
    "\n",
    "In the test set, we would have only the last month of listening for each users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    test_user_id_logs = user_id_logs[user_id_logs['timestamp'] > datetime.datetime(2009, 3, 4).replace(tzinfo=datetime.timezone.utc)]\n",
    "    train_user_id_logs = user_id_logs[user_id_logs['timestamp'] < datetime.datetime(2009, 3, 4).replace(tzinfo=datetime.timezone.utc)]\n",
    "except:\n",
    "    test_user_id_logs = user_id_logs[user_id_logs['timestamp'] > datetime.datetime(2009, 3, 4)]\n",
    "    train_user_id_logs = user_id_logs[user_id_logs['timestamp'] < datetime.datetime(2009, 3, 4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape : ((8519568, 6) and test shape : ((613400, 6))\n"
     ]
    }
   ],
   "source": [
    "print(f'train shape : ({train_user_id_logs.shape} and test shape : ({test_user_id_logs.shape})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    val_user_id_logs = train_user_id_logs[train_user_id_logs['timestamp'] > datetime.datetime(2009, 1, 4).replace(tzinfo=datetime.timezone.utc)]\n",
    "    train_user_id_logs = train_user_id_logs[train_user_id_logs['timestamp'] < datetime.datetime(2009, 1, 4).replace(tzinfo=datetime.timezone.utc)]\n",
    "except:\n",
    "    val_user_id_logs = train_user_id_logs[train_user_id_logs['timestamp'] > datetime.datetime(2009, 1, 4)]\n",
    "    train_user_id_logs = train_user_id_logs[train_user_id_logs['timestamp'] < datetime.datetime(2009, 1, 4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape : ((7975441, 6) and val shape : ((544126, 6))\n"
     ]
    }
   ],
   "source": [
    "print(f'train shape : ({train_user_id_logs.shape} and val shape : ({val_user_id_logs.shape})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's only take the n most listened songs of each users**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_only_top(df_logs,df_profile, n_top):\n",
    "    new_df = pd.DataFrame(columns = ['track-name','artist-name'], dtype= np.str)\n",
    "    for user_id in df_profile.values:\n",
    "        test = df_logs[df_logs['userid']== user_id]\n",
    "        try:\n",
    "            test['count'] = test.groupby(['track-id'])[['track-id']].transform(lambda x: x.count())['track-id']\n",
    "            test = test.sort_values(by = 'count', ascending = False)\n",
    "            test = test.drop('timestamp', axis = 1)\n",
    "            test = test.drop_duplicates()\n",
    "            new_df = pd.concat([new_df, test[:n_top]], ignore_index=True)\n",
    "        except:\n",
    "            pass\n",
    "        clear_output(wait = True)\n",
    "        print(\"Just finished for\",user_id)\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>track-name</th>\n",
       "      <th>artist-name</th>\n",
       "      <th>userid</th>\n",
       "      <th>artist-id</th>\n",
       "      <th>track-id</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>43661</th>\n",
       "      <td>Jolene</td>\n",
       "      <td>Cake</td>\n",
       "      <td>user_000949</td>\n",
       "      <td>fa7b9055-3703-473a-8a09-adf2fe031a24</td>\n",
       "      <td>60f0bfa4-8da9-4840-b5fe-23c1fc470f34</td>\n",
       "      <td>1456.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43662</th>\n",
       "      <td>Staring At The Sun</td>\n",
       "      <td>Tv On The Radio</td>\n",
       "      <td>user_000949</td>\n",
       "      <td>eb872766-98f6-453d-883f-2ae908a18315</td>\n",
       "      <td>64581a21-566d-4b24-99ae-c5f48b75e660</td>\n",
       "      <td>1409.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6809</th>\n",
       "      <td>Wings Of Words</td>\n",
       "      <td>Chemistry</td>\n",
       "      <td>user_000141</td>\n",
       "      <td>c8524763-e7d7-4225-8a9a-e7b64db5a1e2</td>\n",
       "      <td>2caf09d4-aad9-5f2d-b66d-616f2c2d0e35</td>\n",
       "      <td>1365.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43663</th>\n",
       "      <td>Heartbeats</td>\n",
       "      <td>The Knife</td>\n",
       "      <td>user_000949</td>\n",
       "      <td>bf710b71-48e5-4e15-9bd6-96debb2e4e98</td>\n",
       "      <td>db4c9220-df76-4b42-b6f5-8bf52cc80f77</td>\n",
       "      <td>1362.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43664</th>\n",
       "      <td>Anthems For A Seventeen Year Old Girl</td>\n",
       "      <td>Broken Social Scene</td>\n",
       "      <td>user_000949</td>\n",
       "      <td>2eada8f8-056a-4093-bbc2-004909ce743b</td>\n",
       "      <td>91951530-d978-4648-95b1-08b1f49ffba5</td>\n",
       "      <td>1352.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  track-name          artist-name  \\\n",
       "43661                                 Jolene                 Cake   \n",
       "43662                     Staring At The Sun      Tv On The Radio   \n",
       "6809                          Wings Of Words            Chemistry   \n",
       "43663                             Heartbeats            The Knife   \n",
       "43664  Anthems For A Seventeen Year Old Girl  Broken Social Scene   \n",
       "\n",
       "            userid                             artist-id  \\\n",
       "43661  user_000949  fa7b9055-3703-473a-8a09-adf2fe031a24   \n",
       "43662  user_000949  eb872766-98f6-453d-883f-2ae908a18315   \n",
       "6809   user_000141  c8524763-e7d7-4225-8a9a-e7b64db5a1e2   \n",
       "43663  user_000949  bf710b71-48e5-4e15-9bd6-96debb2e4e98   \n",
       "43664  user_000949  2eada8f8-056a-4093-bbc2-004909ce743b   \n",
       "\n",
       "                                   track-id   count  \n",
       "43661  60f0bfa4-8da9-4840-b5fe-23c1fc470f34  1456.0  \n",
       "43662  64581a21-566d-4b24-99ae-c5f48b75e660  1409.0  \n",
       "6809   2caf09d4-aad9-5f2d-b66d-616f2c2d0e35  1365.0  \n",
       "43663  db4c9220-df76-4b42-b6f5-8bf52cc80f77  1362.0  \n",
       "43664  91951530-d978-4648-95b1-08b1f49ffba5  1352.0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if not exists('./saved_data/train_user_top_logs.tsv'):\n",
    "    train_user_top_logs = get_only_top(train_user_id_logs,user_id_profile['#id'], n_top = 50)\n",
    "    train_user_top_logs = train_user_top_logs.nlargest(10000,'count')\n",
    "    train_user_top_logs.to_csv('./saved_data/train_user_top_logs.tsv')\n",
    "\n",
    "else : \n",
    "    train_user_top_logs = pd.read_csv('./saved_data/train_user_top_logs.tsv', index_col=0)\n",
    "\n",
    "train_user_top_logs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Transform the dataset into a bipartite graph**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>track-name</th>\n",
       "      <th>artist-name</th>\n",
       "      <th>artist-id</th>\n",
       "      <th>track-id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Music</td>\n",
       "      <td>Cornelius</td>\n",
       "      <td>df765d93-621c-437f-99fe-fda9e135f89a</td>\n",
       "      <td>52bef5e2-17b6-5742-b846-09a6b750e857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Gum</td>\n",
       "      <td>Cornelius</td>\n",
       "      <td>df765d93-621c-437f-99fe-fda9e135f89a</td>\n",
       "      <td>bb9a7981-016d-596e-b17f-ba07a346d2d4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mario Basanov &amp; Vidis ‘Test’</td>\n",
       "      <td>Gilles Peterson</td>\n",
       "      <td>4c4e3121-4d12-4f7a-a77c-5becd849fb3c</td>\n",
       "      <td>7434fb0f-1245-5a58-b343-cca4d0e2c107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Child Song</td>\n",
       "      <td>The Cinematic Orchestra</td>\n",
       "      <td>7c158ea8-c0aa-410e-bdc1-20bba9759577</td>\n",
       "      <td>4562ff4f-b619-5557-8600-87f6d0d9f348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>To Build A Home</td>\n",
       "      <td>The Cinematic Orchestra</td>\n",
       "      <td>7c158ea8-c0aa-410e-bdc1-20bba9759577</td>\n",
       "      <td>dcf825de-85a9-5b53-9d2b-a9d574b57470</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     track-name              artist-name  \\\n",
       "0                         Music                Cornelius   \n",
       "1                           Gum                Cornelius   \n",
       "2  Mario Basanov & Vidis ‘Test’          Gilles Peterson   \n",
       "3                    Child Song  The Cinematic Orchestra   \n",
       "4               To Build A Home  The Cinematic Orchestra   \n",
       "\n",
       "                              artist-id                              track-id  \n",
       "0  df765d93-621c-437f-99fe-fda9e135f89a  52bef5e2-17b6-5742-b846-09a6b750e857  \n",
       "1  df765d93-621c-437f-99fe-fda9e135f89a  bb9a7981-016d-596e-b17f-ba07a346d2d4  \n",
       "2  4c4e3121-4d12-4f7a-a77c-5becd849fb3c  7434fb0f-1245-5a58-b343-cca4d0e2c107  \n",
       "3  7c158ea8-c0aa-410e-bdc1-20bba9759577  4562ff4f-b619-5557-8600-87f6d0d9f348  \n",
       "4  7c158ea8-c0aa-410e-bdc1-20bba9759577  dcf825de-85a9-5b53-9d2b-a9d574b57470  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if not exists('./saved_data/track_df.tsv'):\n",
    "    track_df = train_user_top_logs.copy()\n",
    "    track_df.drop(['count', 'userid'], axis = 1, inplace=True)\n",
    "    track_df.drop_duplicates('track-id', inplace=True)\n",
    "    track_df.reset_index(drop=True, inplace=True)\n",
    "    track_df.to_csv('./saved_data/track_df.tsv')\n",
    "else : \n",
    "    track_df = pd.read_csv('./saved_data/track_df.tsv', index_col=0)\n",
    "\n",
    "track_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist-name</th>\n",
       "      <th>artist-id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cornelius</td>\n",
       "      <td>df765d93-621c-437f-99fe-fda9e135f89a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Gilles Peterson</td>\n",
       "      <td>4c4e3121-4d12-4f7a-a77c-5becd849fb3c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Cinematic Orchestra</td>\n",
       "      <td>7c158ea8-c0aa-410e-bdc1-20bba9759577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Radiohead</td>\n",
       "      <td>a74b1b7f-71a5-4011-9441-d0b5e4122711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A Hundred Birds</td>\n",
       "      <td>f5341587-6c20-4e0f-bdfd-62b2122825f2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               artist-name                             artist-id\n",
       "0                Cornelius  df765d93-621c-437f-99fe-fda9e135f89a\n",
       "1          Gilles Peterson  4c4e3121-4d12-4f7a-a77c-5becd849fb3c\n",
       "2  The Cinematic Orchestra  7c158ea8-c0aa-410e-bdc1-20bba9759577\n",
       "3                Radiohead  a74b1b7f-71a5-4011-9441-d0b5e4122711\n",
       "4          A Hundred Birds  f5341587-6c20-4e0f-bdfd-62b2122825f2"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if not exists('./saved_data/artists_df.tsv'):\n",
    "    artist_df = train_user_top_logs.copy()\n",
    "    artist_df.drop(['count', 'userid', 'track-name', 'track-id'], axis = 1, inplace=True)\n",
    "    artist_df.drop_duplicates('artist-id', inplace=True)\n",
    "    artist_df.reset_index(drop=True, inplace=True)\n",
    "    artist_df.to_csv('./saved_data/artists_df.tsv')\n",
    "else : \n",
    "    artist_df = pd.read_csv('./saved_data/artists_df.tsv', index_col=0)\n",
    "\n",
    "artist_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from networkx.algorithms import bipartite\n",
    "#We transform the train set into a bipartite graph\n",
    "G= nx.Graph()\n",
    "edges_1 = np.array(train_user_top_logs[['userid', 'track-id','count']].values)\n",
    "edges_2 = np.array(train_user_top_logs[['artist-id', 'track-id']].values)\n",
    "G.add_nodes_from(user_id_profile['#id'], bipartite=0)\n",
    "G.add_nodes_from(track_df['track-id'], bipartite=1)\n",
    "G.add_nodes_from(artist_df['artist-id'], bipartite=2)\n",
    "     \n",
    "G.add_weighted_edges_from(edges_1)\n",
    "#weighted graph\n",
    "G.add_edges_from(edges_2)\n",
    "#save the edges\n",
    "nx.write_gml(G, \"saved_data/graph_user_tracks_train.gml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now we transform the validation data set in a bipartite graph**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Just finished for user_001000\n"
     ]
    }
   ],
   "source": [
    "#We transform the val set into a bipartite graph\n",
    "\n",
    "#First we select only the music tracks that are in the train set\n",
    "val_user_id_logs = val_user_id_logs[val_user_id_logs['track-id'].isin(track_df['track-id'])]\n",
    "val_user_id_logs_top = get_only_top(val_user_id_logs,user_id_profile['#id'], n_top = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Then we create the graph\n",
    "\n",
    "G_val = nx.Graph()\n",
    "edges_1 = np.array(val_user_id_logs_top[['userid', 'track-id','count']].values)\n",
    "edges_2 = np.array(train_user_top_logs[['artist-id', 'track-id']].values)\n",
    "\n",
    "G_val.add_nodes_from(user_id_profile['#id'], bipartite=0)\n",
    "G_val.add_nodes_from(track_df['track-id'], bipartite=1)\n",
    "G_val.add_nodes_from(artist_df['artist-id'], bipartite=2)\n",
    "     \n",
    "G_val.add_weighted_edges_from(edges_1)\n",
    "\n",
    "#weighted graph\n",
    "G_val.add_edges_from(edges_2)\n",
    "#save the graph\n",
    "nx.write_gml(G_val, \"saved_data/graph_user_tracks_val.gml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Just finished for user_001000\n"
     ]
    }
   ],
   "source": [
    "#We transform the test set into a bipartite graph\n",
    "\n",
    "#First we select only the music tracks that are in the train set\n",
    "test_user_id_logs = test_user_id_logs[test_user_id_logs['track-id'].isin(track_df['track-id'])]\n",
    "test_user_id_logs_top = get_only_top(test_user_id_logs,user_id_profile['#id'], n_top = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Then we create the graph\n",
    "\n",
    "G_test_gt = nx.Graph()\n",
    "edges_1 = np.array(test_user_id_logs_top[['userid', 'track-id','count']].values)\n",
    "edges_2 = np.array(train_user_top_logs[['artist-id', 'track-id']].values)\n",
    "\n",
    "G_test_gt.add_nodes_from(user_id_profile['#id'], bipartite=0)\n",
    "G_test_gt.add_nodes_from(track_df['track-id'], bipartite=1)\n",
    "G_test_gt.add_nodes_from(artist_df['artist-id'], bipartite=2)\n",
    "     \n",
    "G_test_gt.add_weighted_edges_from(edges_1)\n",
    "\n",
    "#weighted graph\n",
    "G_test_gt.add_edges_from(edges_2)\n",
    "#save the graph\n",
    "nx.write_gml(G_test_gt, \"saved_data/graph_user_tracks_test_groundtruth.gml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the methods\n",
    "Pearson coefficient, Bellman ford algorithm ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get the graph**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_G  = nx.read_gml(\"saved_data/graph_user_tracks_train.gml\")\n",
    "val_G  = nx.read_gml(\"saved_data/graph_user_tracks_val.gml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>track-name</th>\n",
       "      <th>artist-name</th>\n",
       "      <th>userid</th>\n",
       "      <th>artist-id</th>\n",
       "      <th>track-id</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Jolene</td>\n",
       "      <td>Cake</td>\n",
       "      <td>user_000949</td>\n",
       "      <td>fa7b9055-3703-473a-8a09-adf2fe031a24</td>\n",
       "      <td>60f0bfa4-8da9-4840-b5fe-23c1fc470f34</td>\n",
       "      <td>1456.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Staring At The Sun</td>\n",
       "      <td>Tv On The Radio</td>\n",
       "      <td>user_000949</td>\n",
       "      <td>eb872766-98f6-453d-883f-2ae908a18315</td>\n",
       "      <td>64581a21-566d-4b24-99ae-c5f48b75e660</td>\n",
       "      <td>1409.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Wings Of Words</td>\n",
       "      <td>Chemistry</td>\n",
       "      <td>user_000141</td>\n",
       "      <td>c8524763-e7d7-4225-8a9a-e7b64db5a1e2</td>\n",
       "      <td>2caf09d4-aad9-5f2d-b66d-616f2c2d0e35</td>\n",
       "      <td>1365.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Heartbeats</td>\n",
       "      <td>The Knife</td>\n",
       "      <td>user_000949</td>\n",
       "      <td>bf710b71-48e5-4e15-9bd6-96debb2e4e98</td>\n",
       "      <td>db4c9220-df76-4b42-b6f5-8bf52cc80f77</td>\n",
       "      <td>1362.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Anthems For A Seventeen Year Old Girl</td>\n",
       "      <td>Broken Social Scene</td>\n",
       "      <td>user_000949</td>\n",
       "      <td>2eada8f8-056a-4093-bbc2-004909ce743b</td>\n",
       "      <td>91951530-d978-4648-95b1-08b1f49ffba5</td>\n",
       "      <td>1352.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              track-name          artist-name       userid  \\\n",
       "0                                 Jolene                 Cake  user_000949   \n",
       "1                     Staring At The Sun      Tv On The Radio  user_000949   \n",
       "2                         Wings Of Words            Chemistry  user_000141   \n",
       "3                             Heartbeats            The Knife  user_000949   \n",
       "4  Anthems For A Seventeen Year Old Girl  Broken Social Scene  user_000949   \n",
       "\n",
       "                              artist-id                              track-id  \\\n",
       "0  fa7b9055-3703-473a-8a09-adf2fe031a24  60f0bfa4-8da9-4840-b5fe-23c1fc470f34   \n",
       "1  eb872766-98f6-453d-883f-2ae908a18315  64581a21-566d-4b24-99ae-c5f48b75e660   \n",
       "2  c8524763-e7d7-4225-8a9a-e7b64db5a1e2  2caf09d4-aad9-5f2d-b66d-616f2c2d0e35   \n",
       "3  bf710b71-48e5-4e15-9bd6-96debb2e4e98  db4c9220-df76-4b42-b6f5-8bf52cc80f77   \n",
       "4  2eada8f8-056a-4093-bbc2-004909ce743b  91951530-d978-4648-95b1-08b1f49ffba5   \n",
       "\n",
       "    count  \n",
       "0  1456.0  \n",
       "1  1409.0  \n",
       "2  1365.0  \n",
       "3  1362.0  \n",
       "4  1352.0  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Concatenate the train and val sets summing the count for the same track and the same user\n",
    "new_val_user_id_logs_top = pd.concat([train_user_top_logs, val_user_id_logs_top], ignore_index=True)\n",
    "#sum count column for same track and same user\n",
    "new_val_user_id_logs_top.groupby(['userid', 'track-id']).sum('count')\n",
    "new_val_user_id_logs_top.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Then we create the graph\n",
    "\n",
    "G_test = nx.Graph()\n",
    "edges_1 = np.array(new_val_user_id_logs_top[['userid', 'track-id','count']].values)\n",
    "edges_2 = np.array(train_user_top_logs[['artist-id', 'track-id']].values)\n",
    "\n",
    "G_test.add_nodes_from(user_id_profile['#id'], bipartite=0)\n",
    "G_test.add_nodes_from(track_df['track-id'], bipartite=1)\n",
    "G_test.add_nodes_from(artist_df['artist-id'], bipartite=2)\n",
    "     \n",
    "G_test.add_weighted_edges_from(edges_1)\n",
    "\n",
    "#weighted graph\n",
    "G_test.add_edges_from(edges_2)\n",
    "#save the graph\n",
    "nx.write_gml(G_test, \"saved_data/graph_user_tracks_test.gml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We try the methods from TP2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The graph is too big to be used for training and validation\n",
    "# We can remove the low degree nodes\n",
    "def remove_low_degree_nodes(train_graph, val_graph):\n",
    "    t_graph, v_graph = train_graph.copy(), val_graph.copy()\n",
    "    to_be_removed = [x for  x, d in t_graph.nodes(data=True) if (t_graph.degree(x) <= 3 and d['bipartite'] != 2)]\n",
    "    print(len(to_be_removed))\n",
    "    t_graph.remove_nodes_from(to_be_removed)\n",
    "    v_graph.remove_nodes_from(to_be_removed)\n",
    "    return t_graph, v_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can remove artist with no more edges\n",
    "def remove_low_degree_nodes2(test_graph, train_graph, val_graph, G_test_gt):\n",
    "    tst_graph, t_graph, v_graph, G_test_gt = test_graph.copy(), train_graph.copy(), val_graph.copy(), G_test_gt.copy()\n",
    "    to_be_removed = []\n",
    "    for  x, d in t_graph.nodes(data=True):\n",
    "        try:\n",
    "            if d['bipartite'] != 2:\n",
    "                if t_graph.degree(x) <= 3:\n",
    "                    to_be_removed.append(x)\n",
    "        except:\n",
    "            to_be_removed.append(x)\n",
    "    t_graph.remove_nodes_from(to_be_removed)\n",
    "    v_graph.remove_nodes_from(to_be_removed)\n",
    "    tst_graph.remove_nodes_from(to_be_removed)\n",
    "    G_test_gt.remove_nodes_from(to_be_removed)\n",
    "    return tst_graph, t_graph, v_graph, G_test_gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can remove artist with no more edges\n",
    "def remove_low_artists(test_graph, train_graph, val_graph,G_test_gt):\n",
    "    tst_graph, t_graph, v_graph, G_test_gt = test_graph.copy(), train_graph.copy(), val_graph.copy(), G_test_gt.copy()\n",
    "    to_be_removed = [x for  x, d in t_graph.nodes(data=True) if (t_graph.degree(x) ==0 and d['bipartite'] == 2)]\n",
    "    t_graph.remove_nodes_from(to_be_removed)\n",
    "    v_graph.remove_nodes_from(to_be_removed)\n",
    "    tst_graph.remove_nodes_from(to_be_removed)\n",
    "    G_test_gt.remove_nodes_from(to_be_removed)\n",
    "    return tst_graph, t_graph, v_graph,G_test_gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9359, 9359, 9359, 9359)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G_test, train_G,val_G, G_test_gt = remove_low_degree_nodes2(G_test,train_G,val_G, G_test_gt)\n",
    "len(G_test.nodes),len(train_G.nodes),len(val_G.nodes), len(G_test_gt.nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(686, 686, 686, 686)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G_test, train_G,val_G,G_test_gt = remove_low_artists(G_test,train_G,val_G, G_test_gt)\n",
    "len(G_test.nodes),len(train_G.nodes),len(val_G.nodes), len(G_test_gt.nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unsupervised link prediction\n",
    "\n",
    "ALL THE METHODS ARE TO BE PUT IN A UTIL.PY FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preferential_attachement(graph, edges=train_G.edges()):\n",
    "    PA = {}\n",
    "    \n",
    "    for edge in edges:\n",
    "        PA[edge] = graph.degree(edge[0]) * graph.degree(edge[1])\n",
    "        \n",
    "    return PA\n",
    "    \n",
    "pa = preferential_attachement(train_G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Jaccard(graph, edges=train_G.edges()):\n",
    "    Jaccard = {}\n",
    "    # Compute Jaccard metric for each non_edge of the graph\n",
    "    \n",
    "    for edge in edges: \n",
    "        inter_size = len(list(nx.common_neighbors(graph, edge[0], edge[1])))\n",
    "        union_size = len(set(graph[edge[0]]) | set(graph[edge[1]]))\n",
    "\n",
    "        if union_size != 0:\n",
    "            Jaccard[edge] = inter_size / union_size\n",
    "        else : \n",
    "            Jaccard[edge] = 0\n",
    "\n",
    "    \n",
    "    return Jaccard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AdamicAdar(graph, edges=train_G.edges()):\n",
    "    AdamicAdar = {}\n",
    "    \n",
    "    for edge in edges: \n",
    "        inter_list = nx.common_neighbors(graph, edge[0], edge[1])\n",
    "        AdamicAdar[edge] = sum( [1/np.log(graph.degree(node)) for node in inter_list] )\n",
    "    \n",
    "    return AdamicAdar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_items([(('user_000787', '0e938086-1f83-5242-944b-7315de233b57'), 198), (('user_000610', '487023bc-e43f-5739-b62d-d634b0e12346'), 154), (('user_000610', '587e8902-dd97-5f4b-b86a-ecf97cdc24a7'), 154), (('user_000714', '4350fd70-bfce-531c-b9f9-1bd7261697ff'), 133), (('user_000714', 'a863f1e4-9c63-5920-af72-f5ab39179dd8'), 133), (('user_000610', '2b81406d-5d45-5710-b177-598f5b3942ab'), 132), (('user_000610', 'e8f0781d-5c0f-5d78-bc63-9f05ba93f6fd'), 132), (('user_000610', '72368dce-d68a-5e76-95e7-f783751419f2'), 132), (('user_000493', '90744886-c2db-590b-bfb6-50d409cddabf'), 128), (('user_000787', '96b089dc-8abd-58ab-b188-283814072b22'), 126)])\n"
     ]
    }
   ],
   "source": [
    "def predict_edges(metric, k):\n",
    "    \n",
    "    # Shuffle randomly entries of dictionnary \n",
    "    l = list(metric.items())\n",
    "    np.random.seed(10) # fix random seed to obtain same random shuffling when repeating experiment\n",
    "    np.random.shuffle(l)\n",
    "    metric = dict(l)\n",
    "\n",
    "    # Retrieve top k value \n",
    "    metric = dict(sorted(metric.items(), key=lambda x:x[1], reverse=True)[:k])\n",
    "    print(metric.items())\n",
    "\n",
    "predict_edges(pa, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "#1 hour to compute NOT TRIED -- (but the result in the state of the art is bad)\n",
    "def evaluation(G_train, G_val):\n",
    "    R = G_val.copy()\n",
    "    R.remove_edges_from(e for e in G_val.edges if e in G_train.edges)\n",
    "    gt = R.edges\n",
    "    k = len(gt)\n",
    "    print(k)\n",
    "    print(\"Starting predictions\")\n",
    "    # --- Apply each method defined above and calculate its accuracy ---\n",
    "    methods = ['Jaccard', 'AdamicAdar', 'preferential_attachement']\n",
    "    \n",
    "    # For each method, compute the similarity scores between all non-edges\n",
    "    # Predict k node pairs with highest score \n",
    "    # Compute accuracy wrt edges actually removed \n",
    "    for method in methods: \n",
    "        res = eval(method)(G_train, nx.non_edges(G_train))\n",
    "        print(\"predicting 4 real \")\n",
    "        pred = sorted(res.items(), key = lambda x:x[1], reverse=True)[:k]\n",
    "        print(\"I finished predicting\")\n",
    "        pred = [el[0] for el in pred]\n",
    "        #print('pred', pred)\n",
    "        #print('gt',gt)\n",
    "        accuracy = len(set(pred).intersection(set(gt))) / k\n",
    "        print(method, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1275\n",
      "Starting predictions\n",
      "predicting 4 real \n",
      "I finished predicting\n",
      "Jaccard 0.0\n",
      "predicting 4 real \n",
      "I finished predicting\n",
      "AdamicAdar 0.0\n",
      "predicting 4 real \n",
      "I finished predicting\n",
      "preferential_attachement 0.007058823529411765\n"
     ]
    }
   ],
   "source": [
    "evaluation(train_G, val_G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Supervized link prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score,recall_score,precision_score\n",
    "import collections\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id_profile = pd.get_dummies(user_id_profile, columns = ['gender','country']).drop(['age','registered'], axis = 1 )\n",
    "user_id_profile.set_index('#id', inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extractor(graph, samples):\n",
    "    \"\"\"\n",
    "    Creates a feature vector for each edge of the graph contained in samples \n",
    "    \"\"\"\n",
    "    feature_vector = []\n",
    "    \n",
    "    # --- Extract manually diverse features relative to each edge contained in samples --- \n",
    "    # Fill in the blanks\n",
    "\n",
    "    # Degree Centrality measure\n",
    "    deg_centrality = nx.degree_centrality(graph)\n",
    "    \n",
    "    # Betweeness centrality measure\n",
    "    betweeness_centrality = nx.betweenness_centrality(graph)\n",
    "\n",
    "    # PageRank measure\n",
    "    page_rank = nx.pagerank(graph)\n",
    "\n",
    "    # Closeness centrality measure\n",
    "    closeness_centrality = nx.closeness_centrality(graph)\n",
    "\n",
    "    # Resource Allocation measure\n",
    "    #resource_allocation = nx.resource_allocation_index(graph)\n",
    "\n",
    "    # --- Extract features relative to each edge contained in samples ---\n",
    "\n",
    "    for edge in tqdm(samples):\n",
    "        source_node, target_node = edge[0], edge[1]\n",
    "\n",
    "        # Degree Centrality\n",
    "        source_degree_centrality = deg_centrality[source_node]\n",
    "        target_degree_centrality = deg_centrality[target_node]\n",
    "        \n",
    "        # Betweeness centrality measure \n",
    "        diff_bt = betweeness_centrality[target_node] - betweeness_centrality[source_node]\n",
    "\n",
    "        # Closeness centrality measure\n",
    "        diff_cl = closeness_centrality[target_node] - closeness_centrality[source_node]\n",
    "\n",
    "        # Preferential Attachement \n",
    "        pref_attach = list(nx.preferential_attachment(graph, [(source_node, target_node)]))[0][2]\n",
    "\n",
    "        # Get info on the user\n",
    "        source_user_info = user_id_profile.loc[source_node].values\n",
    "\n",
    "        # Distance between the two nodes\n",
    "        try:\n",
    "            distance = nx.shortest_path_length(graph, source_node, target_node)\n",
    "        except:\n",
    "            distance = 0\n",
    "\n",
    "        # AdamicAdar\n",
    "        #aai = list(nx.adamic_adar_index(graph, [(source_node, target_node)]))[0][2]\n",
    "\n",
    "        # Jaccard\n",
    "        #jacard_coeff = list(nx.jaccard_coefficient(graph, [(source_node, target_node)]))[0][2]\n",
    "\n",
    "        # PageRank\n",
    "        source_page_rank = page_rank[source_node]\n",
    "        \n",
    "        # Create edge feature vector with all metric computed above\n",
    "        feature_vector.append(np.array([source_degree_centrality, target_degree_centrality, \n",
    "                                        diff_bt, pref_attach, source_page_rank, diff_cl, distance] + source_user_info.tolist())) \n",
    "        \n",
    "    return feature_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sets(graph):\n",
    "    pos_sample=list(graph.edges)\n",
    "    neg_sample=list(nx.non_edges(graph))\n",
    "    labels = [1 for _ in pos_sample] + [0 for _ in neg_sample]\n",
    "    features=pos_sample+neg_sample\n",
    "    return features,labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Lets try with only detecting for non edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_neg_edges_bipartite(G):\n",
    "    top_nodes = set(n for n,d in G.nodes(data=True) if d['bipartite']==0)\n",
    "    low_nodes = set(n for n,d in G.nodes(data=True) if d['bipartite']==1)\n",
    "    adj_matrix = bipartite.biadjacency_matrix(G, row_order=top_nodes, column_order=low_nodes)\n",
    "\n",
    "    negative_edges = []\n",
    "    top = list(top_nodes)\n",
    "    low = list(low_nodes)\n",
    "    for i in tqdm(range(len(top_nodes))):\n",
    "        for j in range(len(low_nodes)):\n",
    "            if adj_matrix[i,j]==0:\n",
    "                negative_edges.append([top[i],low[j]])\n",
    "    return negative_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 352/352 [00:01<00:00, 185.49it/s]\n"
     ]
    }
   ],
   "source": [
    "from networkx.algorithms import bipartite\n",
    "train_neg_edges = get_neg_edges_bipartite(train_G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_edges_values(G, edges):\n",
    "    all_positive_edges = G.edges()\n",
    "    output = []\n",
    "    for edge in edges:\n",
    "        if(edge in all_positive_edges): \n",
    "            #output.append(G.edges[edge[0],edge[1]]['weight'])\n",
    "            output.append(1)  \n",
    "        else:\n",
    "            output.append(0)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labelling(G, edges):\n",
    "    all_positive_edges = G.edges()\n",
    "    all_neg_edges=nx.non_edges(G)\n",
    "    all_edges = all_positive_edges + all_neg_edges\n",
    "    output=[1 for i in all_positive_edges ] + [0 for i in all_neg_edges ]\n",
    "   \n",
    "    return output,all_positive_edges,all_neg_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1275"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_links = get_edges_values(val_G, train_neg_edges)\n",
    "new_links.count(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 77352/77352 [00:11<00:00, 6576.66it/s]\n"
     ]
    }
   ],
   "source": [
    "# --- Create feature vector for all edges in training set and test set ---\n",
    "train_features = feature_extractor(train_G, train_neg_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier, BaggingClassifier, ExtraTreesClassifier, GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_features, train_labels):\n",
    "    \"\"\"\n",
    "    Downstream ML task using edge embeddings to classify them \n",
    "    \"\"\"\n",
    "    \n",
    "    # --- Build the model and train it ---\n",
    "    # Fill in the blanks\n",
    "    clf = GradientBoostingClassifier(random_state= 1999, n_estimators=100, learning_rate=0.1, max_depth=5)\n",
    "\n",
    "    clf.fit(train_features, train_labels)\n",
    "\n",
    "    #compute accuracy\n",
    "    train_preds=clf.predict(train_features)\n",
    "\n",
    "    print(collections.Counter(np.array(train_preds)))\n",
    "    acc=accuracy_score(train_labels,train_preds)\n",
    "    recall=recall_score(train_labels,train_preds)\n",
    "    precision=precision_score(train_labels,train_preds)\n",
    "\n",
    "    print('Accuracy:',acc, 'Recall:',recall, 'Precision:',precision)\n",
    "    \n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 77179, 1: 173})\n",
      "Accuracy: 0.9852104664391353 Recall: 0.1192156862745098 Precision: 0.8786127167630058\n"
     ]
    }
   ],
   "source": [
    "RF_model = train(train_features, new_links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We test the model on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_N_experiment(features,edges, N, clf):\n",
    "    \"\"\"\n",
    "    Compute the top N edges in the graph G using the classifier clf\n",
    "    \"\"\"\n",
    "    # --- Predict the labels of the edges ---\n",
    "    preds = clf.predict_proba(features)\n",
    "\n",
    "    # --- Get the top N edges ---\n",
    "    top_N_edges = []\n",
    "    for i in range(N):\n",
    "        top_N_edges.append(edges[np.argmax(preds[:,1])])\n",
    "        preds[np.argmax(preds[:,1])] = -1\n",
    "\n",
    "    return top_N_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 352/352 [04:09<00:00,  1.41it/s]\n"
     ]
    }
   ],
   "source": [
    "test_neg_edges = np.array(get_neg_edges_bipartite(G_test))\n",
    "precisions = []\n",
    "recalls = []\n",
    "for user in tqdm(set(test_neg_edges[:,0])):\n",
    "    user_edges = test_neg_edges[test_neg_edges[:,0]==user]\n",
    "    user_labels = get_edges_values(G_test_gt, user_edges)\n",
    "    if user_labels.count(1) != 0:\n",
    "        user_features = feature_extractor(G_test, user_edges)\n",
    "        top_N_edges = top_N_experiment(user_features, user_edges, 50, RF_model)\n",
    "        top_N_edges = [list(edge) for edge in top_N_edges]\n",
    "        user_edges = [list(edge) for edge in user_edges]\n",
    "        '''print(top_N_edges)\n",
    "        print(user_edges)'''\n",
    "        #prediction = user_edges[user_edges in top_N_edges].astype(int)\n",
    "        prediction = []\n",
    "        for i in range(len(user_edges)):\n",
    "            if user_edges[i] in top_N_edges:\n",
    "                prediction.append(1)\n",
    "            else:\n",
    "                prediction.append(0)\n",
    "\n",
    "        print(prediction.count(1), prediction.count(0))\n",
    "\n",
    "\n",
    "        print(user_labels.count(1))\n",
    "        precision = precision_score(user_labels, prediction)\n",
    "        recall = recall_score(user_labels, prediction)\n",
    "        print(\"Precision : \",precision,\" Recall : \", recall)\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "    clear_output(wait = True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.023925233644859812"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#3.9 % top 10\n",
    "#2.7 % top 20\n",
    "np.mean(precisions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, test_features, test_labels):\n",
    "    \"\"\"\n",
    "    Downstream ML task using edge embeddings to classify them \n",
    "    \"\"\"\n",
    "\n",
    "    #compute accuracy\n",
    "    test_preds=model.predict(test_features)\n",
    "\n",
    "    print(collections.Counter(np.array(test_preds)))\n",
    "    acc=accuracy_score(test_labels,test_preds)\n",
    "    recall=recall_score(test_labels,test_preds)\n",
    "    precision=precision_score(test_labels,test_preds)\n",
    "\n",
    "    print('Accuracy:',acc, 'Recall:',recall, 'Precision:',precision)\n",
    "    return test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 352/352 [00:01<00:00, 180.10it/s]\n",
      "100%|██████████| 76065/76065 [00:11<00:00, 6490.84it/s]\n"
     ]
    }
   ],
   "source": [
    "test_neg_edges = get_neg_edges_bipartite(G_test)\n",
    "# --- Create feature vector for all edges in training set and test set ---\n",
    "test_features = feature_extractor(G_test, test_neg_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = get_edges_values(G_test_gt, test_neg_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "632"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels.count(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 74901, 1: 1164})\n",
      "Accuracy: 0.976835601130612 Recall: 0.02689873417721519 Precision: 0.014604810996563574\n"
     ]
    }
   ],
   "source": [
    "preds = predict(RF_model,test_features,test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.9919923 , 0.0080077 ],\n",
       "       [0.97712059, 0.02287941],\n",
       "       [0.9938698 , 0.0061302 ],\n",
       "       ...,\n",
       "       [0.85943043, 0.14056957],\n",
       "       [0.87213615, 0.12786385],\n",
       "       [0.85943043, 0.14056957]])"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RF_model.predict_proba(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 75589, 1: 476})\n",
      "Accuracy: 0.9978965358574903 Recall: 0.75 Precision: 0.9957983193277311\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BaggingClassifier(random_state=1999)"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train(test_features,test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 75693, 1: 372})\n",
      "Accuracy: 0.9868533491093144 Recall: 0.0031645569620253164 Precision: 0.005376344086021506\n"
     ]
    }
   ],
   "source": [
    "predict(RF_model,test_features,test_labels)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
